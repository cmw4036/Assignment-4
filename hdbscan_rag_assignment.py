# -*- coding: utf-8 -*-
"""HDBSCAN_RAG_Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15cY_hwe32qbwClhPMPRaLCW_xgtw85j4

# HDBSCAN Clustering + Advanced Retrieval (RAG-style) Assignment

This notebook has two parts:

**Part 1:** HDBSCAN Clustering — Deep Dive into Algorithm Internals  
**Part 2:** Advanced Retrieval Systems and RAG (Retrieval-Augmented Generation)
"""

!pip -q install hdbscan umap-learn sentence-transformers faiss-cpu

import re
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

import umap
import hdbscan
import faiss

from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

sns.set(style="whitegrid")
np.random.seed(42)

!pip -q install -U google-genai

from google.colab import userdata
from google import genai

api_key = userdata.get("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)

models = list(client.models.list())
print("Number of models:", len(models))

for m in models[:30]:
    print(m.name)

from google.colab import userdata
from google import genai

api_key = userdata.get("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)

MODEL_NAME = "models/gemini-2.5-flash"  # stable + fast (you have it)

resp = client.models.generate_content(
    model=MODEL_NAME,
    contents="Respond with exactly: OK"
)
print(resp.text)

"""## Part 1: HDBSCAN Clustering — Deep Dive into Algorithm Internals

### 1.1 Load and Prepare the Dataset
We load the CSV and create a single text field by combining `title` and `content`.  
This is common in text clustering because titles are short but informative, while content provides additional context.
"""

DATA_PATH = "/content/combined_data_environment.csv"

df = pd.read_csv(
    DATA_PATH,
    sep=";",
    encoding="latin1",
    engine="python",
    on_bad_lines="skip"
)

print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
df.head()

def clean_text(x):
    x = "" if pd.isna(x) else str(x)
    x = re.sub(r"\s+", " ", x).strip()
    return x

df["title_clean"] = df["title"].apply(clean_text) if "title" in df.columns else ""
df["content_clean"] = df["content"].apply(clean_text) if "content" in df.columns else ""

df["text"] = (df["title_clean"] + ". " + df["content_clean"]).str.strip()
df = df[df["text"].str.len() > 30].copy()   # remove very tiny/empty rows
df = df.drop_duplicates(subset=["text"]).reset_index(drop=True)

print("After cleaning:", df.shape)
df[["title_clean","text"]].head()

"""### 1.2 Create Embeddings
HDBSCAN works on numeric vectors. For text, we first convert documents into dense vectors (“embeddings”).  
We use a pretrained `SentenceTransformer` model (`all-MiniLM-L6-v2`) because it is fast and works well for semantic similarity.
"""

N = min(1000, len(df))
df_small = df.sample(n=N, random_state=42).reset_index(drop=True)

model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(df_small["text"].tolist(), show_progress_bar=True, normalize_embeddings=True)

embeddings = np.array(embeddings).astype("float32")
print("Embeddings shape:", embeddings.shape)

"""### 1.3 Dimensionality Reduction (UMAP)
HDBSCAN can work in high dimensions, but it often performs better after reducing dimensionality.
UMAP preserves local structure and helps clustering + visualization.
"""

reducer = umap.UMAP(
    n_neighbors=15,
    n_components=5,   # 5D for clustering stability
    min_dist=0.0,
    metric="cosine",
    random_state=42
)

emb_umap = reducer.fit_transform(embeddings)
emb_umap = emb_umap.astype("float32")
print("UMAP shape:", emb_umap.shape)

"""### 1.4 Run HDBSCAN
HDBSCAN is a density-based clustering algorithm.
Key parameters:
- `min_cluster_size`: smallest cluster size allowed
- `min_samples`: how strict density should be (higher = more noise points)
"""

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=15,
    min_samples=5,
    metric="euclidean",
    prediction_data=True
)

labels = clusterer.fit_predict(emb_umap)
df_small["cluster"] = labels

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = (labels == -1).sum()

print("Clusters found:", n_clusters)
print("Noise points:", n_noise)
df_small["cluster"].value_counts().head(10)

reducer_2d = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric="cosine", random_state=42)
emb_2d = reducer_2d.fit_transform(embeddings)

plt.figure(figsize=(10,6))
plt.scatter(emb_2d[:,0], emb_2d[:,1], c=labels, cmap="tab20", s=10)
plt.title("HDBSCAN Clusters (UMAP 2D Projection)")
plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.show()

"""### 1.5 Algorithm Internals (Deep Dive)

HDBSCAN builds clusters using a density-based idea:

1. **Core distance**: distance from a point to its k-th nearest neighbor (controls local density).
2. **Mutual reachability distance** between points *a* and *b*:
   - max(core_dist(a), core_dist(b), dist(a,b))
   This makes sparse areas “farther apart” and dense areas “closer”.
3. Using mutual reachability, HDBSCAN constructs a **minimum spanning tree (MST)** and then forms a hierarchy.
4. Clusters are selected based on **stability** over different density levels (condensed tree).

Below we visualize the condensed tree which represents the cluster hierarchy and stability.
"""

clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())
plt.show()

"""### 1.6 Core Distance and Mutual Reachability

To better understand HDBSCAN internals, I compute:

- **Core distance**: distance from each point to its k-th nearest neighbor (k is similar to `min_samples`).
- **Mutual reachability distance** between points a and b:

\[
d_{mr}(a,b) = \max(\text{core}_k(a), \text{core}_k(b), d(a,b))
\]

This transforms the space so sparse regions are pushed apart and dense regions are kept closer, which helps build the MST and cluster hierarchy.
"""

from sklearn.neighbors import NearestNeighbors

k = 10  # choose k close to min_samples (or a bit larger)
nbrs = NearestNeighbors(n_neighbors=k, metric="euclidean").fit(emb_umap)
distances, _ = nbrs.kneighbors(emb_umap)

core_dist = distances[:, -1]
df_small["core_dist"] = core_dist

plt.figure(figsize=(8,4))
sns.histplot(core_dist, bins=40, kde=True)
plt.title(f"Core Distance Distribution (k={k})")
plt.xlabel("Core distance")
plt.show()

from sklearn.metrics import pairwise_distances

# sample for speed
sample_n = min(200, len(emb_umap))
idx = np.random.choice(len(emb_umap), size=sample_n, replace=False)

X = emb_umap[idx]
D = pairwise_distances(X, metric="euclidean")   # shape (n,n)

core_sample = core_dist[idx]                    # shape (n,)

# mutual reachability: max(D, core_i, core_j)
MR = np.maximum(D, core_sample[:, None])
MR = np.maximum(MR, core_sample[None, :])

plt.figure(figsize=(6,5))
sns.heatmap(MR[:40, :40], cmap="viridis")
plt.title("Mutual Reachability (Sample 40x40)")
plt.show()

clusterer.single_linkage_tree_.plot(cmap="viridis", colorbar=True)
plt.title("HDBSCAN Single Linkage Tree (MST-based hierarchy)")
plt.show()

pers = clusterer.cluster_persistence_
print("Cluster persistence:", pers)

plt.figure(figsize=(8,4))
plt.bar(range(len(pers)), pers)
plt.title("Cluster Persistence (Stability)")
plt.xlabel("Cluster index")
plt.ylabel("Persistence")
plt.show()

"""### 1.7 Interpret Clusters (Top Keywords)
To understand what each cluster represents, we extract top TF‑IDF keywords from documents within each cluster.
This helps explain the “theme” of each group.
"""

def top_keywords_for_cluster(texts, top_n=8):
    vec = TfidfVectorizer(stop_words="english", max_features=3000)
    X = vec.fit_transform(texts)
    avg = np.asarray(X.mean(axis=0)).ravel()
    idx = avg.argsort()[::-1][:top_n]
    return [vec.get_feature_names_out()[i] for i in idx]

clusters = sorted([c for c in df_small["cluster"].unique() if c != -1])

for c in clusters[:5]:  # show first 5 clusters (fast)
    texts_c = df_small.loc[df_small["cluster"] == c, "text"].tolist()
    print(f"\nCluster {c} | size={len(texts_c)}")
    print("Top keywords:", top_keywords_for_cluster(texts_c, top_n=10))
    print("Example title:", df_small.loc[df_small["cluster"] == c, "title_clean"].iloc[0])

"""### 1.8 Hyperparameter Optimization (Mini Grid Search)

I test a small grid over (`min_cluster_size`, `min_samples`) and compare:
- number of clusters
- noise ratio
- mean persistence (stability)

This helps justify the final parameter choice.
"""

grid_mcs = [10, 20, 40]
grid_ms = [1, 5, 10]

rows = []
for mcs in grid_mcs:
    for ms in grid_ms:
        c = hdbscan.HDBSCAN(min_cluster_size=mcs, min_samples=ms)
        lab = c.fit_predict(emb_umap)

        n_clusters = len(set(lab)) - (1 if -1 in lab else 0)
        noise_ratio = float(np.mean(lab == -1))
        mean_persist = float(np.mean(c.cluster_persistence_)) if len(c.cluster_persistence_) > 0 else 0.0

        rows.append({
            "min_cluster_size": mcs,
            "min_samples": ms,
            "n_clusters": n_clusters,
            "noise_ratio": noise_ratio,
            "mean_persistence": mean_persist
        })

grid_df = pd.DataFrame(rows).sort_values(["mean_persistence","noise_ratio"], ascending=[False, True])
grid_df

pivot_p = grid_df.pivot(index="min_cluster_size", columns="min_samples", values="mean_persistence")
plt.figure(figsize=(6,4))
sns.heatmap(pivot_p, annot=True, fmt=".2f", cmap="YlGnBu")
plt.title("Mean Persistence by HDBSCAN Parameters")
plt.show()

pivot_n = grid_df.pivot(index="min_cluster_size", columns="min_samples", values="noise_ratio")
plt.figure(figsize=(6,4))
sns.heatmap(pivot_n, annot=True, fmt=".2f", cmap="Reds")
plt.title("Noise Ratio by HDBSCAN Parameters")
plt.show()

"""## Part 2: Advanced Retrieval Systems and RAG

In this section I implement:
1) **Hybrid Search** (BM25 + semantic vector search)  
2) **Cross-Encoder Reranking** (two-stage retrieval)  
3) **RAG with Citations** using Gemini (LLM + retrieved evidence)
"""

!pip -q install rank-bm25 sentence-transformers faiss-cpu

def chunk_text(text, chunk_size=350, overlap=80):
    words = str(text).split()
    if len(words) < 40:
        return []
    chunks = []
    step = chunk_size - overlap
    for i in range(0, len(words), step):
        ch = " ".join(words[i:i+chunk_size])
        if len(ch.split()) >= 40:
            chunks.append(ch)
    return chunks

# Use your df (or df_small) depending on what you used for clustering
# For speed, we use a subset
N_DOCS = min(600, len(df))
df_rag = df.sample(N_DOCS, random_state=42).copy()

# Build a single text field
df_rag["title_clean"] = df_rag["title"].fillna("").astype(str)
df_rag["content_clean"] = df_rag["content"].fillna("").astype(str)
df_rag["text"] = (df_rag["title_clean"] + ". " + df_rag["content_clean"]).str.strip()

chunks, meta = [], []
for doc_id, row in df_rag.reset_index(drop=True).iterrows():
    doc_chunks = chunk_text(row["text"])
    for chunk_id, ch in enumerate(doc_chunks):
        chunks.append(ch)
        meta.append({"doc_id": doc_id, "chunk_id": chunk_id, "title": row["title_clean"]})

print("Docs used:", len(df_rag))
print("Total chunks:", len(chunks))
print("Example chunk:\n", chunks[0][:400])

"""### 2.2 Build Vector Index (FAISS)
We embed each chunk and store the vectors in a FAISS index.  
FAISS allows fast approximate nearest neighbor search (vector similarity search).
"""

chunk_emb = model.encode(chunks, show_progress_bar=True, normalize_embeddings=True)
chunk_emb = np.array(chunk_emb).astype("float32")

dim = chunk_emb.shape[1]
index = faiss.IndexFlatIP(dim)   # inner product (works with normalized embeddings = cosine similarity)
index.add(chunk_emb)

print("FAISS index size:", index.ntotal)

"""### 2.3 Hybrid Search (BM25 + Semantic)
We combine lexical BM25 and semantic vector search by normalizing both scores and using a weighted sum (alpha).
"""

!pip -q install rank-bm25
from rank_bm25 import BM25Okapi

tokenized_corpus = [c.lower().split() for c in chunks]
bm25 = BM25Okapi(tokenized_corpus)

def normalize_scores(arr):
    arr = np.array(arr, dtype=float)
    if arr.max() == arr.min():
        return np.zeros_like(arr)
    return (arr - arr.min()) / (arr.max() - arr.min())

def hybrid_retrieve(query, k=10, alpha=0.6):
    # semantic
    q_emb = model.encode([query], normalize_embeddings=True).astype("float32")
    sem_scores, sem_idxs = index.search(q_emb, k)
    sem_scores, sem_idxs = sem_scores[0], sem_idxs[0]

    # bm25
    bm_scores_all = bm25.get_scores(query.lower().split())

    sem_norm = normalize_scores(sem_scores)
    bm_norm_all = normalize_scores(bm_scores_all)

    combined = []
    for s_norm, idx in zip(sem_norm, sem_idxs):
        comb = alpha * s_norm + (1 - alpha) * bm_norm_all[idx]
        combined.append((float(comb), int(idx)))

    combined.sort(reverse=True, key=lambda x: x[0])
    top = combined[:k]

    return [(score, idx, meta[idx], chunks[idx]) for score, idx in top]

q = "green bonds climate finance"
hyb = hybrid_retrieve(q, k=5, alpha=0.6)
for score, idx, m, ch in hyb:
    print("\nHybrid:", round(score,3), "| cite:", f"[{m['doc_id']}:{m['chunk_id']}]")
    print("Title:", m["title"][:80])
    print(ch[:250])

"""### 2.4 Cross-Encoder Reranking (Two-Stage Retrieval)
We rerank the hybrid results using a cross-encoder that scores (query, chunk) pairs more accurately.
"""

from sentence_transformers import CrossEncoder
cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

def rerank_with_cross_encoder(query, retrieved, top_k=5):
    pairs = [(query, r[3]) for r in retrieved]
    ce_scores = cross_encoder.predict(pairs)

    reranked = []
    for (hyb_score, idx, m, ch), ce in zip(retrieved, ce_scores):
        reranked.append((float(ce), float(hyb_score), idx, m, ch))

    reranked.sort(reverse=True, key=lambda x: x[0])
    return reranked[:top_k]

reranked = rerank_with_cross_encoder(q, hyb, top_k=5)

print("BEFORE (Hybrid order):")
for score, idx, m, ch in hyb:
    print(f"  {score:.3f} -> [{m['doc_id']}:{m['chunk_id']}] {m['title'][:60]}")

print("\nAFTER (Cross-Encoder order):")
for ce, hyb_score, idx, m, ch in reranked:
    print(f"  CE={ce:.3f} (hyb={hyb_score:.3f}) -> [{m['doc_id']}:{m['chunk_id']}] {m['title'][:60]}")

"""### 2.5 RAG Pipeline with Citations (Gemini)
We pass the top reranked chunks into Gemini and force citations in the format [doc_id:chunk_id].
"""

def build_context_from_reranked(reranked_results, max_chars=6000):
    blocks, total = [], 0
    for ce, hyb_score, idx, m, ch in reranked_results:
        cite = f"[{m['doc_id']}:{m['chunk_id']}]"
        block = f"{cite}\n{ch}"
        if total + len(block) > max_chars:
            break
        blocks.append(block)
        total += len(block)
    return "\n\n---\n\n".join(blocks)

def gemini_rag(question, reranked_results):
    context = build_context_from_reranked(reranked_results)
    prompt = f"""
Answer the question using ONLY the context below.

Rules:
- If the context is insufficient, say: "I cannot answer from the provided documents."
- Every factual claim must end with a citation like [doc_id:chunk_id].
- Do not invent citations.

QUESTION:
{question}

CONTEXT:
{context}
""".strip()

    resp = client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt
    )
    return resp.text

def full_rag_pipeline(question, alpha=0.6, k_retrieve=10, k_rerank=5):
    retrieved = hybrid_retrieve(question, k=k_retrieve, alpha=alpha)
    reranked = rerank_with_cross_encoder(question, retrieved, top_k=k_rerank)
    answer = gemini_rag(question, reranked)
    return answer, reranked

questions = [
    "What does the dataset report about green bonds or green bond issuance?",
    "What challenges are discussed about sustainable finance or climate finance?",
    "What examples of climate-related initiatives are mentioned?"
]

for q in questions:
    ans, src = full_rag_pipeline(q)
    print("\n" + "="*80)
    print("Q:", q)
    print("A:", ans)

"""### 2.6 Baseline Semantic Retrieval (FAISS Top-k)
This is a baseline semantic search system using FAISS only. Later sections add BM25 hybrid search and cross-encoder reranking.
"""

def retrieve(query, k=5):
    q_emb = model.encode([query], normalize_embeddings=True).astype("float32")
    scores, idxs = index.search(q_emb, k)
    results = []
    for score, idx in zip(scores[0], idxs[0]):
        results.append((float(score), meta[idx], chunks[idx]))
    return results

query = "green finance and renewable energy projects"
results = retrieve(query, k=5)

for score, m, ch in results:
    print("\nScore:", round(score, 3), "| doc:", m["doc_id"], "| chunk:", m["chunk_id"])
    print("Title:", m["title"][:120])
    print("Chunk preview:", ch[:400])